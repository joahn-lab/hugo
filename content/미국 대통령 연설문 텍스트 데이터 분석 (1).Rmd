---
title: "미국 대통령 연설문 텍스트 데이터 분석 (1)"
author: "joahn-lab"
date: '2020 6 1'
output: html_document
---

## 도입

# 데이터 호출 및 전처리
  UC Santa Barbara의 'The American Presidency project'에서는 매해 단위로 수집한 역대 미국 대통령의 의회 국정연설문을 텍스트 데이터로 아카이빙하고 있다. 'State of the Union'(SOTU) 데이터셋이 바로 그것인데, 초대 대통령 조지 워싱턴(1789~1797 재임)부터 45대 대통령 도널드 트럼프(2017~)의 구두/서면 연설문이 txt 파일 형태로 정리되어 있다. URL 역시 크롤링하기 간편한 규칙성을 띠고 있어 txt 파일데이터 전량을 손쉽게 크롤링할 수 있다. 
  URL 링크 접근과 크롤링을 위해 정리된 메타데이터 엑셀 파일과, 정리된 연설문 파일을 불러오는 것부터 시작하자.
  
```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(tm)
library(quanteda)
```

```{r, results='hide'}
mylocation = "C:/Users/Byeongjun Cho/Desktop/2020-1/데이터사이언스입문/data/STU_address"
setwd(mylocation)
list.files()
```

```{r}
stu = read.csv("STU_address_metadata.csv",header=T) %>% as_tibble()
head(stu)
```

  이번 텍스트 데이터 전처리는 tidytext 패키지의 unnest_tokens() 함수를 이용한다. 연설문 텍스트 파일을 tibble 형태로 불러온 후, 연결된 문장을 단어별로 구획해보자.
  
```{r}
doc_1 = read_lines("doc_1.txt") %>% as_tibble() %>% 
  unnest_tokens(words, value, token="words")

head(doc_1, n = 10)
```

# 평균 단어 수 계산
 
  보기와 같이 띄어쓰기를 기준으로 한 어절 단위로 연설문이 처리되었음을 알 수 있다. 이제 우리는 해당 텍스트 데이터의 기본적인 분석에 착수할 수 있다. 우선 연설문당 평균 단어수를 계산해보자.

```{r, results='hide'}
tmp = stu %>% as_tibble() %>%
  select(president, doc_id, president_no) %>%
  arrange(doc_id)

wnum = list()
for (i in 1:max(tmp$doc_id)){
  doc_n = read_lines(str_c("doc_",as.character(i),".txt")) %>% as_tibble %>%
    unnest_tokens(words, value, token = "words")
  wnum = bind_rows(wnum, count(doc_n))
}
anyNA(cbind(tmp, wnum))
A1 = cbind(tmp, wnum) %>% as_tibble

meanword = list()
mw = list()
pname = list()
tmpname = list()
for (i in 1:42){
mw = mean(A1$n[A1$president == unique(A1$president)[i]])
meanword[[i]] = mw
tmpname = as.character(unique(A1$president)[i])
pname[[i]] = tmpname
}
a = as.data.frame(cbind(meanword, pname))
a = a[nrow(a):1,]
```

이때 22대, 24대 대통령은 'Grover Cleveland'로, 연속 연임 대통령이 아닌 유일한 2선 대통령이므로 별도의 전처리를 거쳐야 한다.

```{r cleveland}
round(mean(A1$n[A1$president_no == "22nd"]),0) # 22대 대통령 재임시절 Cleveland
round(mean(A1$n[A1$president_no == "24th"]),0) # 24대 대통령 재임시절 Cleveland
a$meanword[21] == as.numeric(a$meanword[a$pname == "Grover Cleveland"])
b = round(as.numeric(a$meanword[-21])) %>% as_tibble()
c = b[1:19,]
c[20:22, ] = c(13401, 13668, 14652)
c[23:43,] = b[21:41,]
```

## 인칭대명사 사용 빈도 계산

  1~3인칭 단/복수 인칭대명사의 빈도 역시 우리에게 많은 것을 알려줄 수 있다. 대통령별 각 대명사의 사용 빈도를 알아보자. 이때 unnest_token() 함수로 구분된 어절은 모두 소문자로 시작하므로 결측을 배제하기 위한 별도의 전처리가 필요 없다. 대명사 종류별 수집할 어휘는 다음과 같다. (2인칭의 경우 단/복수 형태의 구분이 없으므로 함께 고려한다.)
 
  1인칭 단수 : i, my, me, mine
  1인칭 복수 : we, us, our, ours
  2인칭 단수/복수 : you, your, yours
  3인칭 단수 : he, she, his, her, him
  3인칭 복수 : they, their, them, theirs
 
```{r}
tmp_2 = tibble()
for (i in 1:max(tmp$doc_id)){
  doc_n = read_lines(str_c("doc_",as.character(i),".txt")) %>% as_tibble %>%
    unnest_tokens(words, value, token = "words")
  tmp_2[i,1] = nrow(doc_n)
  tmp_2[i,2] = sum(length(doc_n$words[doc_n$words == "i"]), length(doc_n$words[doc_n$words == "my"]),
  length(doc_n$words[doc_n$words == "me"]), length(doc_n$words[doc_n$words == "mine"]))
  tmp_2[i,3] = sum(length(doc_n$words[doc_n$words == "we"]), length(doc_n$words[doc_n$words == "our"]),
  length(doc_n$words[doc_n$words == "us"]), length(doc_n$words[doc_n$words == "ours"]))
  tmp_2[i,4] = sum(length(doc_n$words[doc_n$words == "you"]), length(doc_n$words[doc_n$words == "your"]),
  length(doc_n$words[doc_n$words == "yours"]))
  tmp_2[i,5] = sum(length(doc_n$words[doc_n$words == "he"]), length(doc_n$words[doc_n$words == "she"]),
  length(doc_n$words[doc_n$words == "his"]), length(doc_n$words[doc_n$words == "her"]),
  length(doc_n$words[doc_n$words == "him"]))
  tmp_2[i,6] = sum(length(doc_n$words[doc_n$words == "they"]), length(doc_n$words[doc_n$words == "their"]),
  length(doc_n$words[doc_n$words == "them"]), length(doc_n$words[doc_n$words == "theirs"]))
}
A2 = inner_join(A1, tmp_2, c("n"="...1")) %>% unique()

head(A2)
```

```{r, results='hide'}
A2_1 = tibble()
A2_1 = A2 %>%
  mutate(...2 = 100*...2/n,
         ...3 = 100*...3/n,
         ...4 = 100*...4/n,
         ...5 = 100*...5/n,
         ...6 = 100*...6/n)

A2_2 = tibble()
for (i in 1:42){
  A2_2[i,1] = unique(A2_1$president)[i]
  A2_2[i,2] = round(mean(A2_1$n[A2_1$president == unique(A2_1$president)[i]]),0)
  A2_2[i,3] = round(mean(A2_1$...2[A2_1$president == unique(A2_1$president)[i]]),2)
  A2_2[i,4] = round(mean(A2_1$...3[A2_1$president == unique(A2_1$president)[i]]),2)
  A2_2[i,5] = round(mean(A2_1$...4[A2_1$president == unique(A2_1$president)[i]]),2)
  A2_2[i,6] = round(mean(A2_1$...5[A2_1$president == unique(A2_1$president)[i]]),2)
  A2_2[i,7] = round(mean(A2_1$...6[A2_1$president == unique(A2_1$president)[i]]),2)
  } 
A2_2[22,] ## Grover Cleveland
A2_2 = A2_2[-22,]
```
```{r}
A2_2 %>%
  mutate(rown = row_number()) %>% arrange(desc(rown)) %>%
  rename("president" = ...1, "totalwords" = ...2, "I" = ...3, "we" = ...4, "you" = ...5, "he/she" = ...6,          "they" = ...7)
```

## 시각화
 
# 대통령별 평균 단어 수 시각화
 
 우선 처음에 구한 대통령별 연설문 평균 단어 수를 시각화해보자.
 
```{r, echo=FALSE}
A2_3 = tibble()
for (i in 0:42){
  A2_3[i+1,1] = unique(A2_1$president_no)[43-i]
  A2_3[i+1,2] = unique(A2_1$president)[43-i]}
for (i in 1:19){
A2_3[c(i,i+1),2] = A2_3[c(i+1, i),2]}
A2_3[20,2] = unique(A2_1$president)[22]

A3_1 = bind_cols(A2_3, c)
A3_1 = A3_1 %>%
  rename("president_no" = ...1, "president" = ...2) 
```

```{r}
A3_1 = A3_1 %>% 
  mutate(president_no = str_extract(president_no,"[[:digit:]]{1,2}"),
         president_no = as.numeric(president_no))

A3_1 %>%
  ggplot(aes(x = 46-president_no, y = value)) +
  geom_point(size = 2) +
  geom_line() +
  labs(x="President of the United States",
       y="Average number of words addressed") +
  ylim(c(1000,25000)) +
  scale_x_continuous(breaks=46-A3_1$president_no,
                     labels=A3_1$president) +
  theme(axis.text.x.bottom = element_text(angle = 45)) +
  theme_light()
```

 이때, 대통령별 변화량에 주목하기보다 시대 변화에 따른 추이를 보고 싶다면 축을 세로로 돌리면 좀 더 보기 편할 것이다. ggplot 패키지의 coord_flip() 함수를 이용하면 된다. 추가적으로 추세선과 함께 그래프를 보면 전반적인 추이를 검토하기 더욱 편할 것이다. 이때 추세선은 전반적이고 유동적인 추이를 시각화하여 제공하고자 함이 주 사용목적이므로 geom_smooth(method = 'loess')를 이용하자.

```{r}
A3_1 %>%
  ggplot(aes(x = 46-president_no, y = value)) +
  geom_point(size = 2) +
  geom_line() +
  geom_smooth() +
  labs(x="President of the United States",
       y="Average number of words addressed") +
  ylim(c(1000,25000)) +
  scale_x_continuous(breaks=46-A3_1$president_no,
                     labels=A3_1$president) +
  coord_flip() +
  theme_light()
```

# 인칭대명사 시각화

 이번엔 인칭대명사를 시각화해보자. 먼저 1인칭 복수대명사('우리')의 대통령별 사용빈도를 살펴보자. 전반적인 그래프와 코드는 앞서 평균 단어 수에 사용한 방식과 동일하다.
 
```{r}
A4_1 = full_join(A3_1, A2_2, by = c("value" = "...2"))
A2 = A2 %>% 
  mutate(A2_num = as.numeric(str_extract(A2$president_no,"[[:digit:]]{1,2}")))

for (i in 1:43){
  A4_1[i,10] = sum(A2$...3[A2$A2_num == unique(A4_1$president_no)[i]])
}

A4_1 = A4_1[,c(1,2,3,10)] %>% rename('p1_plural'= ...10)
A4_1 %>%
  ggplot(aes(x = 46-president_no, y = p1_plural)) +
  geom_point(size = 2) +
  geom_line() +
  geom_smooth() +
  labs(x="President of the United States",
       y="Average number of 1P Plural form words addressed") +
  ylim(c(0,3500)) +
  scale_x_continuous(breaks=46-A3_1$president_no,
                     labels=A3_1$president) +
  coord_flip() +
  theme_light()
```

우리는 여기서 시대에 따른 '우리' 어휘 사용의 일정한 경향성을 파악할 수 있다. 다른 인칭대명사들의 경우에도 동일할까?


```{r}
```
## 주관적 해석

## 마무리

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
